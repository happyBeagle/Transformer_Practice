# Transformer Practice   

## 🍮 Reference   
* [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)    
* [WIKI docs](https://wikidocs.net/31379)    

## 🍭 Intro   
  Deep Learning 학습을 하거나 대회를 나가보면 `Transformer`기반 모델 혹은 `Transformer`그 자체를 사용하여 성능을 내거나 사용을 하였다는 소리를 많이 듣는다. 그런데 나는 대회에서 혹은 스터디 모임에서 `Transformer`를 사용해 보았지만, 해당 모델에 대한 이해도가 부족하였다는 것을 깨달았다. 그래서 이제 `Transformer`자체에 대해 학습을 해보고자한다.   
 
> Although the Transformer is the most famous model for it's high perfomance in Deep Learning fields, I have been implementing without deep understanding. This repository is for studying the `Transformer`.  

```
$> tree -d
.
├── /config
│     └── configuration files 
├── /modules
│     ├── multihead_attention_layer.py
│     ├── encoder_layer.py
│     ├── decoder_layer.py
│     ├── transformer.py
│     └── utils.py
├── train.py
└── evaluation.py
```   

## 🧁 What Is Transformer?   
* TODO

## 🍰 How to Develop Transformer?   
* TODO   

## 🥧 TODO List   
- [ ] markdown 정리하기   
- [ ] repository 구조 정리하기   
- [ ] transformer 구현하기    
	- [ ] attention 구현하기   
	- [ ] transformer 구현   
	- [ ] training code 구현   
	- [ ] evaluation code 구현   
