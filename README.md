# Transformer Practice   

## ğŸ® Reference   
* [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)    
* [WIKI docs](https://wikidocs.net/31379)    

## ğŸ­ Intro   
  Deep Learning í•™ìŠµì„ í•˜ê±°ë‚˜ ëŒ€íšŒë¥¼ ë‚˜ê°€ë³´ë©´ `Transformer`ê¸°ë°˜ ëª¨ë¸ í˜¹ì€ `Transformer`ê·¸ ìì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë‚´ê±°ë‚˜ ì‚¬ìš©ì„ í•˜ì˜€ë‹¤ëŠ” ì†Œë¦¬ë¥¼ ë§ì´ ë“£ëŠ”ë‹¤. ê·¸ëŸ°ë° ë‚˜ëŠ” ëŒ€íšŒì—ì„œ í˜¹ì€ ìŠ¤í„°ë”” ëª¨ì„ì—ì„œ `Transformer`ë¥¼ ì‚¬ìš©í•´ ë³´ì•˜ì§€ë§Œ, í•´ë‹¹ ëª¨ë¸ì— ëŒ€í•œ ì´í•´ë„ê°€ ë¶€ì¡±í•˜ì˜€ë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤. ê·¸ë˜ì„œ ì´ì œ `Transformer`ìì²´ì— ëŒ€í•´ í•™ìŠµì„ í•´ë³´ê³ ìí•œë‹¤.   
 
> Although the Transformer is the most famous model for it's high perfomance in Deep Learning fields, I have been implementing without deep understanding. This repository is for studying the `Transformer`.  

```
$> tree -d
.
â”œâ”€â”€ /config
â”‚     â””â”€â”€ configuration files 
â”œâ”€â”€ /modules
â”‚     â”œâ”€â”€ multihead_attention_layer.py
â”‚     â”œâ”€â”€ encoder_layer.py
â”‚     â”œâ”€â”€ decoder_layer.py
â”‚     â”œâ”€â”€ transformer.py
â”‚     â””â”€â”€ utils.py
â”œâ”€â”€ train.py
â””â”€â”€ evaluation.py
```   

## ğŸ§ What Is Transformer?   
* TODO

## ğŸ° How to Develop Transformer?   
* TODO   

## ğŸ¥§ TODO List   
- [ ] markdown ì •ë¦¬í•˜ê¸°   
- [ ] repository êµ¬ì¡° ì •ë¦¬í•˜ê¸°   
- [ ] transformer êµ¬í˜„í•˜ê¸°    
	- [ ] attention êµ¬í˜„í•˜ê¸°   
	- [ ] transformer êµ¬í˜„   
	- [ ] training code êµ¬í˜„   
	- [ ] evaluation code êµ¬í˜„   
